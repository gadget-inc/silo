---
title: Deployment
---

To use Silo for job execution, you must set up two things:

- the Silo server, which will store your jobs and broker tasks
- some workers listening to this Silo server that actually run tasks.

Your worker instances will poll the Silo server for new tasks to run, run them locally, and report the outcome back to Silo. On failure, the job will be re-attempted in the future according to the job's retry schedule.

If you want to run multiple Silo instances in a cluster to spread out your load, you must also configure a cluster membership system for Silo. Currently, etcd and Kubernetes API based cluster membership providers are available.

## Object storage vs local disk

Silo's main thing is storing job data in object storage, which avoids the need for stateful compute nodes. Everything gets easier as an operator when you can treat a service like cattle instead of pets, and Silo intends to be cattle.

Object storage has some drawbacks, however -- you must accept either high costs and low latency, or low costs and high latency.

## Shard count

### Impact of shard count on costs

TODO -- more shards means fewer writes per shard

### Impact of shard count on throughput

TODO -- more shards means more throughput, but too much per shard overhead, extra object storage writes / compaction overhead

### Tenants need to be small enough to fit in one shard and not create hotspots


### Using local disk for WAL

For maximum write performance, you can configure the write-ahead log (WAL) to use local disk while keeping the main data in object storage. This avoids the latency of writing every WAL entry to object storage.

```toml
[database]
backend = "gcs"
path = "gs://my-bucket/silo/%shard%"

[database.wal]
backend = "fs"
path = "/var/lib/silo/wal/%shard%"
```

When using a local WAL, data is first written to local disk and then periodically flushed to object storage. On graceful shutdown, `apply_wal_on_close = true` (the default) ensures the WAL is fully flushed to object storage before the shard is closed.

**Crash recovery with permanent shard leases**: If a node crashes before the WAL is flushed, the unflushed data only exists on local disk. Silo uses **permanent shard leases** to protect this data: the crashed node's shard leases persist, preventing any other node from opening the shard and missing the local WAL data. When the node restarts with the same identity (see `node_id` below), it reclaims its shard leases and recovers the WAL from local disk.

For this to work, you need:

1. **Stable node identity**: Set `node_id` in your coordination config so that a restarted node uses the same identity. In Kubernetes with StatefulSets, use `node_id = "${POD_NAME}"`.
2. **Persistent local storage**: The local WAL directory must survive pod restarts. Use a `PersistentVolumeClaim` or local volumes in Kubernetes.

If a node is permanently lost (e.g., the disk is destroyed), an operator must force-release its shard leases using `siloctl shard force-release <shard-id>`. Any unflushed WAL data on that node will be lost.

### Using local disk for object storage cache

TODO

## Using etcd for cluster membership

TODO

## Using Kubernetes for cluster membership

TODO
